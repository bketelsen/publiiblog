<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><title>Organizing Documents with Some AI, ML, and Elbow Grease</title><meta name="description" content="I discuss how I am using machine learning, AI, and good old-fashioned elbow grease to make sense of the 3000 files in my `~/Documents/Unfiled` directory."><meta name="generator" content="Publii Open-Source CMS for Static Site"><link rel="canonical" href="https://publiiblog.vercel.app/organizing-documents-with-some-ai-ml-and-elbow-grease/"><link rel="alternate" type="application/atom+xml" href="https://publiiblog.vercel.app/feed.xml"><link rel="alternate" type="application/json" href="https://publiiblog.vercel.app/feed.json"><meta property="og:title" content="Organizing Documents with Some AI, ML, and Elbow Grease"><meta property="og:image" content="https://publiiblog.vercel.app/media/posts/1/IGa3Md8wP6g.jpg"><meta property="og:site_name" content="Brian Ketelsen"><meta property="og:description" content="I discuss how I am using machine learning, AI, and good old-fashioned elbow grease to make sense of the 3000 files in my `~/Documents/Unfiled` directory."><meta property="og:url" content="https://publiiblog.vercel.app/organizing-documents-with-some-ai-ml-and-elbow-grease/"><meta property="og:type" content="article"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:site" content="@bketelsen"><meta name="twitter:title" content="Organizing Documents with Some AI, ML, and Elbow Grease"><meta name="twitter:description" content="I discuss how I am using machine learning, AI, and good old-fashioned elbow grease to make sense of the 3000 files in my `~/Documents/Unfiled` directory."><meta name="twitter:image" content="https://publiiblog.vercel.app/media/posts/1/IGa3Md8wP6g.jpg"><link rel="shortcut icon" href="https://publiiblog.vercel.app/media/website/bk32.png" type="image/png"><style>:root{--primary-font:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen,Ubuntu,Cantarell,"Fira Sans","Droid Sans","Helvetica Neue",Arial,sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol";--secondary-font:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen,Ubuntu,Cantarell,"Fira Sans","Droid Sans","Helvetica Neue",Arial,sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol"}</style><link rel="stylesheet" href="https://publiiblog.vercel.app/assets/css/fontawesome-all.min.css?v=bbcde81f26378440dac4c3d195714389"><link rel="stylesheet" href="https://publiiblog.vercel.app/assets/css/style.css?v=49cf0e62580c5ae9cb47763ba682a848"><script type="application/ld+json">{"@context":"http://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://publiiblog.vercel.app/organizing-documents-with-some-ai-ml-and-elbow-grease/"},"headline":"Organizing Documents with Some AI, ML, and Elbow Grease","datePublished":"2019-09-03T06:10","dateModified":"2021-01-06T08:27","image":{"@type":"ImageObject","url":"https://publiiblog.vercel.app/media/posts/1/IGa3Md8wP6g.jpg","height":900,"width":1600},"description":"In this first post of (likely) a multi-part series I’m going to discuss how I am using machine learning, AI,&hellip;","author":{"@type":"Person","name":"Brian"},"publisher":{"@type":"Organization","name":"Brian","logo":{"@type":"ImageObject","url":"https://publiiblog.vercel.app/media/website/bri.png","height":1459,"width":1234}}}</script></head><body class="is-preload"><div id="wrapper"><div id="main"><div class="inner"><header id="header"><a class="logo logo-image" href="https://publiiblog.vercel.app/"><img src="https://publiiblog.vercel.app/media/website/bri.png" alt="Brian Ketelsen"></a><ul class="icons"><li><a href="https://twitter.com/bketelsen" class="icon brands fa-twitter"><span class="label">Twitter</span></a></li><li><a href="https://youtube.com/bketelsen" class="icon brands fa-youtube"><span class="label">YouTube</span></a></li></ul></header><article class="post"><header class="main post__header"><time datetime="2019-09-03T06:10" class="post__date">September 3, 2019</time><h1>Organizing Documents with Some AI, ML, and Elbow Grease</h1></header><figure class="image main"><img src="https://publiiblog.vercel.app/media/posts/1/IGa3Md8wP6g.jpg" srcset="https://publiiblog.vercel.app/media/posts/1/responsive/IGa3Md8wP6g-xs.jpg 300w, https://publiiblog.vercel.app/media/posts/1/responsive/IGa3Md8wP6g-sm.jpg 480w, https://publiiblog.vercel.app/media/posts/1/responsive/IGa3Md8wP6g-md.jpg 768w, https://publiiblog.vercel.app/media/posts/1/responsive/IGa3Md8wP6g-lg.jpg 1024w, https://publiiblog.vercel.app/media/posts/1/responsive/IGa3Md8wP6g-xl.jpg 1360w, https://publiiblog.vercel.app/media/posts/1/responsive/IGa3Md8wP6g-xxl.jpg 1600w" sizes="(max-width: 1600px) 100vw, 1600px" loading="lazy" height="900" width="1600" alt="Motorcycle helmet with camera lenses inserted to look like a robot"><figcaption>https://unsplash.com/photos/IGa3Md8wP6g</figcaption></figure><div class="post__inner post__entry"><p>In this first post of (likely) a multi-part series I’m going to discuss how I am using machine learning, AI, and good old-fashioned elbow grease to make sense of the 3000 files in my <code>~/Documents/Unfiled</code> directory.</p><h3 id="the-problem-statement">The Problem Statement</h3><p>There are several contributing factors to the problem. Let’s start with the obvious ones:</p><ul><li>I’m a digital packrat</li><li>I’m a single parent of 3 (and therefore busy)</li><li>I can be lazy sometimes</li><li>I have ADHD, and get easily sidetracked from things I intended to do</li></ul><p>When my dad passed away last year, it got even worse. Suddenly I was getting all of his mail, bills, correspondence, too. I didn’t want to lose it; but I sure wasn’t ready to read it all. So I scanned it and dropped it in the <code>Unfiled</code> folder.</p><p>So now we’re here. Where <code>here</code> is a place where I can’t find anything I need and my <code>Documents</code> directory is the definition of <code>hot-mess</code>.</p><h3 id="the-goal">The Goal</h3><p>I’d like to take that folder of 3000 random unclassified documents and sort them into something more clear. I think sorting them by originating source (Supplier, Vendor, Biller, Organization) is a good first step. Eventually I’d like to sort them by date group too. Probably by Year, then Month.</p><p>For a bonus, I’d love to do a <a href="https://docs.microsoft.com/en-us/windows/win32/projfs/projected-file-system?WT.mc_id=none-twitter-brketels">projected filesystem</a> sort of thing in Windows and a <a href="https://9p.io/wiki/plan9/Installing_a_Plan_9_File_Server/index.html">Plan9</a> type server on Mac/Linux using FUSE. It’d be really convenient to be able to get at documents from a Filesystem interface by using different facets like keywords, dates, categories, etc. That might fit more cleanly with the way I think, too. But, again, that’s a stretch goal, because we’ll need all that metadata first.</p><p>If you’re old enough to remember <a href="https://arstechnica.com/information-technology/2018/07/the-beos-filesystem/">BeOS Filesystem</a>, it would have solved nearly all of this. Someday we’ll get back to the database/filesystem mashup that truly needs to exist.</p><h3 id="the-solutions">The Solution(s)</h3><p>First, there isn’t really a one-step solution to this. It’s going to take some work, and I can likely automate MOST of that. But there will still be a good portion of things I can’t sort automatically.</p><h4 id="step-one">Step One</h4><p>As a first step, I wrote a small Go program that calls <a href="https://cda.ms/126">Azure Cognitive Services</a> Vision API to do Optical Character Recognition on all the files that are compatible (PDF and image files). Nearly everything I have is in pdf format, but there are a few TIFF files in there too. This program is in flux right now, so I’m not going to release it as Open Source until it’s settled a bit. If I forget - ping me on twitter @bketelsen or email <a href="mailto:mail@bjk.fyi">mail@bjk.fyi</a> - and remind me! Related: the code samples in this post are probably garbage, and won’t likely match the end result that I publish. I’m sure I’m swallowing errors, and haven’t done the slightest bit of refactor/cleanup on this code yet.</p><blockquote><p>WARNING: <em>Don’t cut/paste this code yet, please.</em></p></blockquote><p>I created a domain type appropriately called <code>Document</code> that stores metadata about files on disk:</p><pre><code class="language-go">type Document struct {
    Hash         string
    Path         string
    PreviousPath string
    Operation    *CognitiveOperation
    Results      *CognitiveReadResponse
}</code></pre><p>I’ll discuss the fields as they come up, but <code>Path</code> and <code>PreviousPath</code> should be obvious. Current and previous location on disk, so that I can account for file moves with at least a little bit of history.</p><p>The pricing for the OCR is really attractive - as of September, 2019 it is:</p><blockquote><p>0-1M transactions — $1.50 per 1,000 transactions</p></blockquote><p>I know that I’ll be fine tuning the processes that run, and likely running them repeatedly. I wanted to find a way to store the results from the OCR for each document, but I am also aware that I can’t use the document name and path as the canonical key to find the document later, because the goal of this app is to move them and rename them appropriately! So I decided to use a hash of the file contents as a key. <code>SHA256</code> seems to be the right algorithm for file contents, low cost computation, low collision chance. So I created a hash function that calculates the <code>SHA256</code> hash of the document after it is read:</p><pre><code class="language-go">func (d *Document) GetHash() {

    f, err := os.Open(d.Path)
    if err != nil {
        log.Fatal(err)
    }
    defer f.Close()

    h := sha256.New()
    if _, err := io.Copy(h, f); err != nil {
        log.Fatal(err)
    }
    d.Hash = fmt.Sprintf(&quot;%x&quot;, h.Sum(nil))
}</code></pre><p>After getting the results of the OCR operation, I set them in the <code>Document</code> type, then persist the metadata to disk in a hidden directory. Currently that’s <code>~/.classifier/</code> but, as with all of this, it might change in the future.</p><p>The file is stored using the <code>SHA256</code> hash of the contents as the file name, and the <code>Document</code> type is serialized to disk using Go’s efficient and lightweight <code>encoding/gob</code> format. While I’m debugging and playing with this code, I decided to also persist the data in <code>json</code> format so it’s easier to read. Here’s the method on <code>Document</code> that saves/serializes to disk:</p><pre><code class="language-go">func (d *Document) SaveMetadata() error {
  fmt.Println(d.Hash)
  //TODO use new XDG config dir location
  // https://tip.golang.org/pkg/os/#UserConfigDir
    filePath := &quot;/home/bjk/.classifier/&quot; + d.Hash // TODO FILEPATH.JOIN
    fmt.Println(filePath)
    file, err := os.OpenFile(filePath, os.O_TRUNC|os.O_CREATE|os.O_WRONLY, 0644)
    if err != nil {
        return err
    }
    defer file.Close()
    enc := gob.NewEncoder(file)
    err = enc.Encode(d)
    if err != nil {
        return err
    }
    jfilePath := &quot;/home/bjk/.classifier/&quot; + d.Hash + &quot;.json&quot; // TODO FILEPATH.JOIN
    fmt.Println(jfilePath)
    jfile, err := os.OpenFile(jfilePath, os.O_TRUNC|os.O_CREATE|os.O_WRONLY, 0644)
    if err != nil {
        return err
    }
    defer jfile.Close()
    jenc := json.NewEncoder(jfile)
    return jenc.Encode(d)
}</code></pre><p>Lots of bad things happening in there, see above caveats about copying/pasting this code. The important part is the encoding in <code>gob</code> format of the contents of the <code>Document</code> metadata, which is then saved to disk using the <code>SHA256</code> hash as the filename. This is a nice future-proof solution, and provides several benefits.</p><ul><li>If there is already a file with the same name, it’s been processed once.</li><li>If the <code>.Path</code> is different from the document I’m inspecting, I might have an exact duplicate, which is a candidate for (soft) deleting</li><li>It doesn’t matter where the files get moved, as long as the <code>SHA256</code> hash matches, I’ve got the metadata saved already.</li></ul><p>This is a very low-tech metadata database, of sorts. It’s definitely not optimized for real-time use, but instead for batch operations.</p><p>Keeping all the metadata in this format means I can write any number of other tools to read and modify the metadata without worrying too much.</p><h3 id="step-two">Step Two</h3><p>At this point, I have a directory full of unprocessed files and a way to process them once and save the results so I don’t have to re-process them later. It’s time to fire off the processing app. I used <a href="https://github.com/spf13/cobra">cobra</a> to build the command-line utility, so I made the root/naked command do the actual calls to Azure Cognitive Services:</p><pre><code class="language-bash">go build
./classifier</code></pre><p>This iterates over every file in the <code>~/Documents/Unfiled</code> directory, calling Cognitive Services OCR for the file types that are supported. There is no current mechanism to retrieve metadata from other document types (Word documents, text files, etc). That’s a future addition.</p><p>After receiving the results, the responses are serialized using the above mentioned <code>gob</code> serialization into <code>~/.classifier/HASH</code></p><h3 id="classification">Classification</h3><p>Based on the results there are some simple <code>bag of words</code> matches that can be done. Some of the documents I have contain very unique text that is indicative of a particular document type. For example, Bank of America always includes my account number and their address in <code>Wilmington</code>. No other document in my corpus has those two distinct things together, so I can write a simple classifier for all Bank of America documents. I decided to use simple TOML for a configuration file here:</p><pre><code>[[entity]]
name = &quot;Bank of America&quot;
directory = &quot;BOA&quot;
keywords = [&quot;Bank of America&quot;,&quot;12345677889&quot;,&quot;Wilmington&quot;]</code></pre><p>Here, I added a sub-command in <code>cobra</code> so I can classify files without re-posting them to Cognitive Services. So I added the <code>classifier process</code> command:</p><pre><code class="language-bash">./classifier process</code></pre><p>It currently goes through all the files in <code>Unfiled</code> and checks their metadata for matches against the TOML file. This worked perfectly for several of my external correspondents. It took all the documents from <code>Unfiled</code> and placed them in <code>Filed/{directory}</code>.</p><h3 id="what-about-the-rest">What About The Rest?</h3><p>There are many documents that aren’t easily processed this way though. My next inspiration came in the shower (of course). If you squint enough, or are far enough away, all documents from the same entity of the same type look the same. So all my mortgage statements look the same, but the numbers are different.</p><p>I installed ImageMagick, and wrote a script to make a low-resolution thumbnail of each PDF. I made the resolution low enough that the text isn’t readable even if you magnify the image.</p><p>Then I searched for ways to compare images and came across <a href="https://github.com/rivo/duplo">duplo</a>, which appears to do what I need. It does a hash of the image and allows you to compare other documents to that hash to find a similarity score. Using this type of process my next goal is to group similar documents together by searching for ones with matching or close-to-matching image hashes.</p><p>But that’ll be probably next weekend. It’s been really fun doing this much, and I’m looking forward to seeing how much more I can learn as I go!</p><p>Intermediate results:</p><p>Before:</p><pre><code class="language-bash">2846 Files</code></pre><p>After:</p><pre><code class="language-bash">Unfiled\
  2710 Files
Filed\
  136 Files in 2 Directories</code></pre></div><footer class="post__inner post__footer"><p class="post__last-updated">This article was updated on January 6, 2021</p><div class="post__share"><h3>Share post:</h3><a href="https://twitter.com/share?url=https%3A%2F%2Fpubliiblog.vercel.app%2Forganizing-documents-with-some-ai-ml-and-elbow-grease%2F&amp;via=bketelsen&amp;text=Organizing%20Documents%20with%20Some%20AI%2C%20ML%2C%20and%20Elbow%20Grease" class="js-share icon brands fa-twitter" rel="nofollow noopener noreferrer"><span class="label">Twitter</span></a></div><div class="post__tag"><ul><li><strong>Tagged in:</strong></li><li><a href="https://publiiblog.vercel.app/category/ai/">AI</a></li><li><a href="https://publiiblog.vercel.app/category/ml/">ML</a></li><li><a href="https://publiiblog.vercel.app/category/posts/">Posts</a></li></ul></div></footer></article></div></div><div id="sidebar"><div class="inner"><nav id="menu"><header class="major"><h2>Menu</h2></header><ul><li><a href="https://publiiblog.vercel.app/" target="_self">Home</a></li><li><a href="https://publiiblog.vercel.app/category/posts/" target="_self">Blog</a></li></ul></nav><section><header class="major"><h2>Editor Pick's</h2></header><div class="mini-posts"><article><a href="https://publiiblog.vercel.app/organizing-documents-with-some-ai-ml-and-elbow-grease/" class="image fit"><img src="https://publiiblog.vercel.app/media/posts/1/IGa3Md8wP6g.jpg" srcset="https://publiiblog.vercel.app/media/posts/1/responsive/IGa3Md8wP6g-xs.jpg 300w, https://publiiblog.vercel.app/media/posts/1/responsive/IGa3Md8wP6g-sm.jpg 480w, https://publiiblog.vercel.app/media/posts/1/responsive/IGa3Md8wP6g-md.jpg 768w" sizes="(max-width: 768px) 100vw, 33vw" loading="lazy" height="900" width="1600" alt="Motorcycle helmet with camera lenses inserted to look like a robot"></a><h3><a href="https://publiiblog.vercel.app/organizing-documents-with-some-ai-ml-and-elbow-grease/">Organizing Documents with Some AI, ML, and Elbow Grease</a></h3><p>In this first post of (likely) a multi-part series I’m going to discuss how I am using machine learning, AI,&hellip;</p></article></div><ul class="actions"><li><a href="https://publiiblog.vercel.app/category/featured/" class="button">More</a></li></ul></section><footer id="footer"><p class="copyright">© 2021 Brian Ketelsen - All rights reserved</p></footer></div></div></div><script src="https://publiiblog.vercel.app/assets/js/jquery.min.js?v=220afd743d9e9643852e31a135a9f3ae"></script><script src="https://publiiblog.vercel.app/assets/js/browser.min.js?v=c07298dd19048a8a69ad97e754dfe8d0"></script><script src="https://publiiblog.vercel.app/assets/js/breakpoints.min.js?v=81a479eb099e3b187613943b085923b8"></script><script src="https://publiiblog.vercel.app/assets/js/util.min.js?v=cbdaf7c20ac2883c77ae23acfbabd47e"></script><script src="https://publiiblog.vercel.app/assets/js/main.min.js?v=448f4c9d4b4c374766a1cb76c3d88048"></script><script>var images=document.querySelectorAll("img[loading]");for(var i=0;i<images.length;i++){if(images[i].complete){images[i].classList.add("is-loaded")}else{images[i].addEventListener("load",function(){this.classList.add("is-loaded")},false)}};</script></body></html>